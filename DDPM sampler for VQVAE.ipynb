{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelimnaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'  # Prevent OpenMP initialization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch  # Import PyTorch library\n",
    "import torch.nn as nn  # Import neural network module\n",
    "import torch.optim as optim  # Import optimization module\n",
    "import math  # Import math module for log calculations\n",
    "from torchvision import datasets, transforms  # Import datasets and transforms\n",
    "from torchvision.utils import save_image, make_grid  # Import utility to save images\n",
    "import torchvision  # Import torchvision library\n",
    "import matplotlib.pyplot as plt  # Import plotting library\n",
    "import os  # Import os module for file operations\n",
    "import numpy as np  # Import numpy library\n",
    "from torch.utils.data import Dataset  # Add this import at the top\n",
    "from PIL import Image  # Import PIL Image module for image handling\n",
    "import torch.nn.functional as F  # Import PyTorch's functional API for loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set device\n",
    "print(f\"Using device: {device}\")  # Print the device being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net architecture for noise prediction in diffusion models with built-in residual connections, optimized for 128-channel 8x8 input\"\"\"\n",
    "    def __init__(self, in_channels=128, time_dim=256):  # Modified input channels to 128 for 8x8 input\n",
    "        super().__init__()\n",
    "\n",
    "        # Pooling and activation layers used throughout the network\n",
    "        self.pool = nn.MaxPool2d(2)  # Max pooling for downsampling\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  # Bilinear upsampling\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "\n",
    "        # Encoder Block 1 - Input level (8x8)\n",
    "        self.enc1_conv1 = nn.Conv2d(in_channels, 256, 3, padding=1)  # First conv: (N,128,8,8) -> (N,256,8,8)\n",
    "        self.enc1_bn1 = nn.BatchNorm2d(256)  # Normalizes each of the 256 channels independently\n",
    "        self.enc1_conv2 = nn.Conv2d(256, 256, 3, padding=1)  # Second conv: (N,256,8,8) -> (N,256,8,8)\n",
    "        self.enc1_bn2 = nn.BatchNorm2d(256)  # Batch norm after second conv\n",
    "\n",
    "        # Encoder Block 2 - After first pooling (4x4)\n",
    "        self.enc2_conv1 = nn.Conv2d(256, 512, 3, padding=1)  # First conv: (N,256,4,4) -> (N,512,4,4)\n",
    "        self.enc2_bn1 = nn.BatchNorm2d(512)  # Batch norm after first conv\n",
    "        self.enc2_conv2 = nn.Conv2d(512, 512, 3, padding=1)  # Second conv: (N,512,4,4) -> (N,512,4,4)\n",
    "        self.enc2_bn2 = nn.BatchNorm2d(512)  # Batch norm after second conv\n",
    "\n",
    "        # Encoder Block 3 - After second pooling (2x2)\n",
    "        self.enc3_conv1 = nn.Conv2d(512, 1024, 3, padding=1)  # First conv: (N,512,2,2) -> (N,1024,2,2)\n",
    "        self.enc3_bn1 = nn.BatchNorm2d(1024)  # Batch norm after first conv\n",
    "        self.enc3_conv2 = nn.Conv2d(1024, 1024, 3, padding=1)  # Second conv: (N,1024,2,2) -> (N,1024,2,2)\n",
    "        self.enc3_bn2 = nn.BatchNorm2d(1024)  # Batch norm after second conv\n",
    "\n",
    "        # Decoder Block 3 - First upsampling (2x2 -> 4x4)\n",
    "        self.dec3_conv1 = nn.Conv2d(1536, 512, 3, padding=1)  # First conv: (N,1536,4,4) -> (N,512,4,4)\n",
    "        self.dec3_bn1 = nn.BatchNorm2d(512)  # Batch norm after first conv\n",
    "        self.dec3_conv2 = nn.Conv2d(512, 512, 3, padding=1)  # Second conv: (N,512,4,4) -> (N,512,4,4)\n",
    "        self.dec3_bn2 = nn.BatchNorm2d(512)  # Batch norm after second conv\n",
    "\n",
    "        # Decoder Block 2 - Second upsampling (4x4 -> 8x8)\n",
    "        self.dec2_conv1 = nn.Conv2d(768, 256, 3, padding=1)  # First conv: (N,768,8,8) -> (N,256,8,8)\n",
    "        self.dec2_bn1 = nn.BatchNorm2d(256)  # Batch norm after first conv\n",
    "        self.dec2_conv2 = nn.Conv2d(256, 256, 3, padding=1)  # Second conv: (N,256,8,8) -> (N,256,8,8)\n",
    "        self.dec2_bn2 = nn.BatchNorm2d(256)  # Batch norm after second conv\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_conv = nn.Conv2d(256, in_channels, kernel_size=1)  # Final conv: (N,256,8,8) -> (N,128,8,8)\n",
    "\n",
    "        # Time embedding dimension and projection\n",
    "        self.time_dim = time_dim  # Time embedding dimension\n",
    "\n",
    "        # Define MLPs as model parameters\n",
    "        self.time_enc1 = nn.Sequential(nn.Linear(time_dim, 256), nn.SiLU(), nn.Linear(256, 256))  # Time embedding MLP for encoder block 1\n",
    "        self.time_enc2 = nn.Sequential(nn.Linear(time_dim, 512), nn.SiLU(), nn.Linear(512, 512))  # Time embedding MLP for encoder block 2\n",
    "        self.time_enc3 = nn.Sequential(nn.Linear(time_dim, 1024), nn.SiLU(), nn.Linear(1024, 1024))  # Time embedding MLP for encoder block 3\n",
    "        self.time_dec3 = nn.Sequential(nn.Linear(time_dim, 512), nn.SiLU(), nn.Linear(512, 512))  # Time embedding MLP for decoder block 3\n",
    "        self.time_dec2 = nn.Sequential(nn.Linear(time_dim, 256), nn.SiLU(), nn.Linear(256, 256))  # Time embedding MLP for decoder block 2\n",
    "\n",
    "    def get_time_embedding(self, t):\n",
    "        \"\"\"Generate sinusoidal time embedding and project through MLPs for each block\n",
    "\n",
    "        Args:\n",
    "            t: Time tensor of shape (batch_size, 1)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing time embeddings for each block\n",
    "        \"\"\"\n",
    "        half_dim = self.time_dim // 2  # Calculate half dimension for sin/cos embeddings\n",
    "        embeddings = torch.arange(half_dim, device=t.device).float()  # Create position indices\n",
    "        embeddings = torch.exp(-math.log(10000) * embeddings / half_dim)  # Calculate frequency bands\n",
    "        embeddings = t * embeddings.unsqueeze(0)  # Shape: (batch_size, half_dim)\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)  # Shape: (batch_size, time_dim)\n",
    "\n",
    "        # Use the class MLPs instead of creating new ones\n",
    "        t_emb_enc1 = self.time_enc1(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_enc2 = self.time_enc2(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_enc3 = self.time_enc3(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_dec3 = self.time_dec3(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_dec2 = self.time_dec2(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "\n",
    "        return {\n",
    "            'enc1': t_emb_enc1,  # Time embedding for encoder block 1\n",
    "            'enc2': t_emb_enc2,  # Time embedding for encoder block 2\n",
    "            'enc3': t_emb_enc3,  # Time embedding for encoder block 3\n",
    "            'dec3': t_emb_dec3,  # Time embedding for decoder block 3\n",
    "            'dec2': t_emb_dec2   # Time embedding for decoder block 2\n",
    "        }\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"Forward pass through U-Net optimized for 8x8 input with time embeddings at each block\"\"\"\n",
    "        # Time embedding\n",
    "        t = t.unsqueeze(-1).float()  # Ensure time is in correct shape\n",
    "        t_embs = self.get_time_embedding(t)  # Get time embeddings for each block\n",
    "\n",
    "        # Encoder pathway with skip connections and time embeddings\n",
    "        # Encoder Block 1 (8x8)\n",
    "        e1 = self.relu(self.enc1_bn1(self.enc1_conv1(x)))  # First conv layer\n",
    "        e1 = self.relu(self.enc1_bn2(self.enc1_conv2(e1)))  # Second conv layer with ReLU\n",
    "        e1 = e1 + t_embs['enc1']  # Add time embedding to encoder block 1\n",
    "\n",
    "        # Encoder Block 2 (4x4)\n",
    "        e2 = self.relu(self.enc2_bn1(self.enc2_conv1(self.pool(e1))))  # First conv layer\n",
    "        e2 = self.relu(self.enc2_bn2(self.enc2_conv2(e2)))  # Second conv layer with ReLU\n",
    "        e2 = e2 + t_embs['enc2']  # Add time embedding to encoder block 2\n",
    "\n",
    "        # Encoder Block 3 (2x2)\n",
    "        e3 = self.relu(self.enc3_bn1(self.enc3_conv1(self.pool(e2))))  # First conv layer\n",
    "        e3 = self.relu(self.enc3_bn2(self.enc3_conv2(e3)))  # Second conv layer with ReLU\n",
    "        e3 = e3 + t_embs['enc3']  # Add time embedding to encoder block 3\n",
    "\n",
    "        # Decoder pathway using skip connections\n",
    "        # Decoder Block 3 (4x4)\n",
    "        d3 = torch.cat([self.upsample(e3), e2], dim=1)  # Concatenate along channel dimension\n",
    "        d3 = self.relu(self.dec3_bn1(self.dec3_conv1(d3)))  # First conv block\n",
    "        d3 = self.dec3_bn2(self.dec3_conv2(d3))  # Second conv block\n",
    "        d3 = d3 + t_embs['dec3']  # Add time embedding to decoder block 3\n",
    "\n",
    "        # Decoder Block 2 (8x8)\n",
    "        d2 = torch.cat([self.upsample(d3), e1], dim=1)  # Concatenate along channel dimension\n",
    "        d2 = self.relu(self.dec2_bn1(self.dec2_conv1(d2)))  # First conv block\n",
    "        d2 = self.dec2_bn2(self.dec2_conv2(d2))  # Second conv block\n",
    "        d2 = d2 + t_embs['dec2']  # Add time embedding to decoder block 2\n",
    "\n",
    "        return self.final_conv(d2)  # Return final output (N,128,8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward diffusion process gradually adds Gaussian noise to an tensor over multiple timesteps. However we can get the noisy tensor directly from the original tensor without going through the intermediate steps without any model as follows:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha_t}} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha_t}} \\cdot \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $x_t$ is the noisy tensor at timestep $t$\n",
    "- $x_0$ is the original clean tensor  \n",
    "- $\\bar{\\alpha_t}$ (alpha_bar) is the cumulative product of $\\alpha_i = (1-\\beta_i)$ up to timestep $t$, i.e. $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$\n",
    "- $\\epsilon$ (epsilon) is random Gaussian noise\n",
    "\n",
    "This process gradually transforms the data distribution into pure Gaussian noise at $t=T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BETA_START = 0.0001  # Start value for noise schedule\n",
    "BETA_END = 0.02  # End value for noise schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def add_noise_at_timestep(x_start, t, timesteps=1000):\n",
    "    \"\"\"\n",
    "    Add noise to tensor at timestep t according to diffusion process\n",
    "\n",
    "    Args:\n",
    "        x_start (torch.Tensor): Original tensor\n",
    "        t (torch.Tensor): Timesteps\n",
    "        timesteps (int): Total number of diffusion steps\n",
    "\n",
    "    Returns:\n",
    "        tuple: Noisy tensor and noise\n",
    "    \"\"\"\n",
    "    device = x_start.device  # Get device from input tensor\n",
    "    noise = torch.randn_like(x_start)  # Generate random noise on same device as x_start\n",
    "\n",
    "    betas = torch.linspace(BETA_START, BETA_END, timesteps).to(device)  # Move noise schedule to device\n",
    "    alphas = 1 - betas  # Alpha values\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)  # Cumulative product of alphas\n",
    "\n",
    "    # Extract relevant alpha values for timestep t\n",
    "    sqrt_alphas_cumprod_t = alphas_cumprod[t].sqrt()  # Get sqrt(alpha_bar) for timestep t\n",
    "    sqrt_one_minus_alphas_cumprod_t = (1 - alphas_cumprod[t]).sqrt()  # Get sqrt(1-alpha_bar) for timestep t\n",
    "\n",
    "    # Reshape for broadcasting\n",
    "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(-1, 1, 1, 1)  # Shape: (batch_size, 1, 1, 1)\n",
    "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t.view(-1, 1, 1, 1)  # Shape: (batch_size, 1, 1, 1)\n",
    "\n",
    "    # Apply noise using the diffusion equation\n",
    "    noisy_images = sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise  # Forward diffusion step\n",
    "\n",
    "    return noisy_images, noise  # Return both noisy images and the noise added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its simply mean square error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def diffusion_loss_fn(model, x_start, timesteps=1000):\n",
    "    \"\"\"\n",
    "    Calculate the diffusion loss across multiple timesteps for each image in batch\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The UNet model for noise prediction\n",
    "        x_start (torch.Tensor): Original clean tensor (batch_size, channels, height, width)\n",
    "        timesteps (int): Total number of diffusion steps # Number of diffusion steps\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Mean loss per image in batch (batch_size,)\n",
    "    \"\"\"\n",
    "    batch_size = x_start.shape[0] # Get batch size from input images\n",
    "\n",
    "    # Sample random timesteps for each image in the batch\n",
    "    t = torch.randint(1, timesteps, (batch_size,), device=x_start.device) # Shape: (batch_size,)\n",
    "\n",
    "    # Add noise to the input images for the sampled timesteps\n",
    "    noisy_images, noise = add_noise_at_timestep(x_start, t, timesteps) # Shapes: (batch_size, channels, H, W)\n",
    "\n",
    "    # Predict the noise using the model\n",
    "    predicted_noise = model(noisy_images, t) # Shape: (batch_size, channels, H, W)\n",
    "\n",
    "    # Calculate MSE loss between predicted and actual noise per image\n",
    "    loss = F.mse_loss(predicted_noise, noise, reduction='none') # Shape: (batch_size, channels, H, W)\n",
    "    loss = loss.mean(dim=(1,2,3)) # Shape: (batch_size,) - Average over channels, height, width\n",
    "\n",
    "    return loss # Return loss per image in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define VQ VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1),  # Input: N x 3 x 128 x 128, Output: N x 64 x 64 x 64, 64 filters of size 4x4x3\n",
    "            nn.LeakyReLU(0.2),  # Applies Leaky ReLU activation function with negative slope of 0.2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),  # Input: N x 64 x 64 x 64, Output: N x 128 x 32 x 32, 128 filters of size 4x4x64\n",
    "            nn.BatchNorm2d(128),  # Applies Batch Normalization to the output of the previous layer\n",
    "            nn.LeakyReLU(0.2),  # Applies Leaky ReLU activation function with negative slope of 0.2\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),  # Input: N x 128 x 32 x 32, Output: N x 256 x 16 x 16, 256 filters of size 4x4x128\n",
    "            nn.BatchNorm2d(256),  # Applies Batch Normalization to the output of the previous layer\n",
    "            nn.LeakyReLU(0.2),  # Applies Leaky ReLU activation function with negative slope of 0.2\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),  # Input: N x 256 x 16 x 16, Output: N x 512 x 8 x 8, 512 filters of size 4x4x256\n",
    "            nn.BatchNorm2d(512),  # Applies Batch Normalization to the output of the previous layer\n",
    "            nn.LeakyReLU(0.2),  # Applies Leaky ReLU activation function with negative slope of 0.2\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Conv2d(in_channels=512, out_channels=latent_dim, kernel_size=3, stride=1, padding=1)  # Input: N x 512 x 8 x 8, Output: N x latent_dim x 8 x 8, latent_dim filters of size 3x3x512\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)  # Apply main encoder layers\n",
    "        latents = self.final_conv(encoded)  # Generate latent vectors\n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim  # Dimension of each embedding vector\n",
    "        self.num_embeddings = num_embeddings  # Number of embedding vectors in the codebook\n",
    "        self.commitment_cost = commitment_cost  # Coefficient for the commitment loss\n",
    "\n",
    "        # Initialize the embedding vectors (codebook)\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)  # Creates an embedding layer to store the codebook\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)  # Initialize embedding weights uniformly\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Convert inputs from BCHW to BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()  # Rearrange dimensions from BCHW to BHWC\n",
    "\n",
    "        input_shape = inputs.shape  # Store original input shape\n",
    "\n",
    "        # Reshape inputs to (batch_size * height * width, channels)\n",
    "        flat_input = inputs.view(-1, self.embedding_dim)  # Flatten input to 2D tensor\n",
    "\n",
    "        # Compute L2 distances between flattened input and embedding vectors\n",
    "        distances = torch.sum(flat_input**2, dim=1, keepdim=True) + \\\n",
    "                    torch.sum(self.embedding.weight**2, dim=1) - \\\n",
    "                    2 * torch.matmul(flat_input, self.embedding.weight.t())  # Calculate distances using the formula: ||x-y||^2 = ||x||^2 + ||y||^2 - 2x^T y\n",
    "\n",
    "        # Find nearest embedding for each input vector\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)  # Find index of nearest embedding for each input vector\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)  # Create one-hot encodings\n",
    "        encodings.scatter_(1, encoding_indices, 1)  # Set the corresponding index to 1 for each encoding\n",
    "\n",
    "        # Quantize the input vectors\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)  # Multiply encodings with embedding weights and reshape to original input shape\n",
    "\n",
    "        # Compute the VQ Losses\n",
    "        commitment_loss = F.mse_loss(quantized.detach(), inputs)  # Commitment loss: how far are the inputs from their quantized values\n",
    "        embedding_loss = F.mse_loss(quantized, inputs.detach())  # Embedding loss: how far are the quantized values from the inputs\n",
    "        vq_loss = embedding_loss + self.commitment_cost * commitment_loss  # Total VQ loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()  # Add quantization error to input (detached to avoid backpropagation through this path)\n",
    "\n",
    "        # Convert quantized from BHWC back to BCHW\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()  # Rearrange dimensions from BHWC back to BCHW\n",
    "\n",
    "        # Compute perplexity\n",
    "        avg_probs = torch.mean(encodings, dim=0)  # Average probability of each encoding across the batch\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))  # Compute perplexity (add small epsilon to avoid log(0))\n",
    "\n",
    "        return vq_loss, quantized, perplexity, encodings  # Return VQ loss, quantized vectors, perplexity, and encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim  # Store the latent dimension for use in the forward pass\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 64, kernel_size=4, stride=2, padding=1),  # Input: N x latent_dim x 8 x 8, Output: N x 64 x 16 x 16, 64 filters of size 4x4xlatent_dim\n",
    "            nn.ReLU(),  # Apply ReLU activation to introduce non-linearity\n",
    "            nn.BatchNorm2d(64),  # Normalize the output to stabilize training\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Input: N x 64 x 16 x 16, Output: N x 32 x 32 x 32, 32 filters of size 4x4x64\n",
    "            nn.ReLU(),  # Apply ReLU activation to introduce non-linearity\n",
    "            nn.BatchNorm2d(32),  # Normalize the output to stabilize training\n",
    "\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # Input: N x 32 x 32 x 32, Output: N x 16 x 64 x 64, 16 filters of size 4x4x32\n",
    "            nn.ReLU(),  # Apply ReLU activation to introduce non-linearity\n",
    "            nn.BatchNorm2d(16),  # Normalize the output to stabilize training\n",
    "\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),  # Input: N x 16 x 64 x 64, Output: N x 3 x 128 x 128, 3 filters of size 4x4x16\n",
    "            nn.Sigmoid()  # Apply Sigmoid activation to ensure output is in range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        decoded_image = self.decoder(x)  # Pass the input through the decoder layers\n",
    "        return decoded_image  # Return the decoded RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings=512, commitment_cost=0.25):\n",
    "        super(VQVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(embedding_dim)  # Initialize the encoder with the embedding dimension as latent dimension\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)  # Initialize the Vector Quantizer with default or provided values\n",
    "        self.decoder = Decoder(embedding_dim)  # Initialize the decoder with the embedding dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)  # Encode the input\n",
    "        vq_loss, quantized, perplexity, _ = self.vq_layer(z)  # Apply Vector Quantization\n",
    "        x_recon = self.decoder(quantized)  # Decode the quantized representation\n",
    "\n",
    "        return vq_loss, x_recon, perplexity  # Return VQ loss, reconstructed image, and perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the saved model from .pth file\n",
    "model_path = r'D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\vqvae_model_epoch_30.pth'  # Use absolute path to model file\n",
    "try:\n",
    "    loaded_model = torch.load(model_path)  # Attempt to load the entire model including architecture and weights\n",
    "    print(f\"Model loaded successfully from {model_path}\")  # Confirm successful load\n",
    "except OSError as e:\n",
    "    print(f\"Error loading model: {e}\")  # Print the specific error message\n",
    "    print(\"Please check if the file path is correct and the file exists.\")\n",
    "    raise  # Re-raise the exception to stop execution if the model can't be loaded\n",
    "\n",
    "# Adjust these parameters to match the saved model\n",
    "latent_dim = 128  # Define the latent dimension for the VQVAE model\n",
    "num_embeddings = 1024  # Define the number of embeddings for the VQ layer\n",
    "embedding_dim = 128  # Define the dimension of each embedding\n",
    "\n",
    "# Create the model with the correct parameters\n",
    "VQVAE_model = VQVAE(embedding_dim=embedding_dim, num_embeddings=num_embeddings)  # Initialize the VQVAE model with specified parameters\n",
    "\n",
    "# Load the state dictionary\n",
    "VQVAE_model.load_state_dict(torch.load(model_path))  # Load the saved model weights into the initialized model\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "VQVAE_model = VQVAE_model.to(device)  # Move model to GPU if available\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "VQVAE_model.eval()  # Set the model to evaluation mode, disabling dropout and using evaluation behavior for batch normalization\n",
    "\n",
    "print(VQVAE_model)  # Print the model architecture to verify it's loaded correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get latent representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_vq_representations(dataloader, VQVAE_model, device):\n",
    "    \"\"\"\n",
    "    Generate vector quantized representations from a VQVAE model\n",
    "\n",
    "    Args:\n",
    "        dataloader: DataLoader containing images\n",
    "        VQVAE_model: Trained VQVAE model\n",
    "        device: Device to run computations on (CPU/GPU)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of quantized tensors # Comment explaining return value\n",
    "    \"\"\"\n",
    "    vq_representations = [] # Initialize list to store VQ representations\n",
    "\n",
    "    VQVAE_model.eval() # Set model to evaluation mode for inference\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient computation for efficiency\n",
    "        for batch_idx, (images, _) in enumerate(dataloader): # Iterate through batches, ignore labels\n",
    "            images = images.to(device) # Move batch of images to device\n",
    "\n",
    "            # Get encoded representations\n",
    "            encoded = VQVAE_model.encoder(images) # Encode images through encoder\n",
    "\n",
    "            # Apply vector quantization\n",
    "            _, quantized, _, _ = VQVAE_model.vq_layer(encoded) # Get quantized vectors from VQ layer\n",
    "\n",
    "            # Store results\n",
    "            vq_representations.append(quantized.cpu().numpy()) # Add quantized representations to list\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0: # Print progress every 10 batches\n",
    "                print(f\"Processed {batch_idx + 1} batches...\") # Print progress message\n",
    "\n",
    "    # Combine all batches\n",
    "    vq_representations = np.concatenate(vq_representations, axis=0) # Stack all VQ representations into single array\n",
    "\n",
    "    # Print final shape\n",
    "    print(f\"Final shape of VQ representations: {vq_representations.shape}\") # Print shape of VQ tensor\n",
    "\n",
    "    return vq_representations # Return VQ representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir  # Directory containing all images\n",
    "        self.transform = transform  # Transformations to apply to images\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]  # List all image files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)  # Return the total number of images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])  # Get path of image at index idx\n",
    "        image = Image.open(img_path).convert('RGB')  # Open image and convert to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations if any\n",
    "\n",
    "        return image, 0  # Return image and a dummy label (0)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize the image to 128x128 pixels\n",
    "    transforms.ToTensor(),  # Convert the PIL Image to a tensor, scales to [0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Butterfly dataset from local machine\n",
    "data_dir = r'D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\Butterfly dataset'  # Path to the Butterfly dataset\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomImageDataset(root_dir=data_dir, transform=transform)  # Use our custom dataset class\n",
    "batch_size = 64  # Set batch size to 64\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)  # Create dataloader with batch size of 64\n",
    "\n",
    "print(f\"Loaded {len(dataset)} images.\")  # Print the total number of images loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get VQ representations for all images\n",
    "print(\"Getting VQ representations for all images...\")  # Print status message\n",
    "\n",
    "all_vq_representations = []  # Initialize list to store VQ representations\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_idx, (images, _) in enumerate(dataloader):  # Iterate through batches\n",
    "        images = images.to(device)  # Move images to device\n",
    "\n",
    "        # Get encoded representations directly\n",
    "        encoded = VQVAE_model.encoder(images)  # Encode images using loaded VQVAE model\n",
    "        _, quantized, _, _ = VQVAE_model.vq_layer(encoded)  # Get quantized vectors\n",
    "\n",
    "        # Store results\n",
    "        all_vq_representations.append(quantized.cpu().numpy())  # Add to list\n",
    "        print(f\"Processed batch {batch_idx+1}/{len(dataloader)}\")  # Print progress\n",
    "\n",
    "# Concatenate all batches into single array\n",
    "final_vq_representations = np.concatenate(all_vq_representations, axis=0)  # Combine all batches\n",
    "\n",
    "print(f\"Final VQ representations shape: {final_vq_representations.shape}\")  # Print final shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize UNet model for image segmentation and move to device\n",
    "u_net_model = UNet()  # Create UNet model instance for image segmentation\n",
    "u_net_model = u_net_model.to(device)  # Move model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "u_net_model.train()  # Enable training mode for U-Net model (activates dropout, batch norm, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "num_epochs = 100  # Number of epochs to train for\n",
    "learning_rate = 1e-4  # Learning rate for optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(u_net_model.parameters(), lr=learning_rate)  # Initialize Adam optimizer for U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset and dataloader for VQ representations\n",
    "vq_dataset = torch.utils.data.TensorDataset(torch.from_numpy(final_vq_representations))  # Create dataset from VQ representations\n",
    "vq_dataloader = torch.utils.data.DataLoader(vq_dataset, batch_size=32, shuffle=True)  # Create dataloader with batch size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop implementation\n",
    "u_net_model.train()  # Set UNet to training mode\n",
    "\n",
    "for epoch in range(num_epochs):  # Iterate through epochs\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")  # Print current epoch progress\n",
    "    epoch_losses = []  # Track losses for this epoch\n",
    "\n",
    "    for batch_idx, (vq_batch,) in enumerate(vq_dataloader):  # Iterate through VQ representation batches\n",
    "        optimizer.zero_grad()  # Reset gradients for this batch\n",
    "        vq_batch = vq_batch.to(device).float()  # Move VQ representations to device and ensure float type\n",
    "\n",
    "        # Calculate diffusion loss using the modified loss function\n",
    "        batch_losses = diffusion_loss_fn(u_net_model, vq_batch)  # Use u_net_model instead of VQVAE model\n",
    "        loss = batch_losses.mean()  # Average loss across batch\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Track and print progress\n",
    "        epoch_losses.append(loss.item())  # Store loss value\n",
    "        if (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Batch {batch_idx+1}, Loss: {loss.item():.4f}\")  # Print current batch loss\n",
    "\n",
    "    # Print epoch summary\n",
    "    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)  # Calculate average epoch loss\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_epoch_loss:.4f}\")  # Print epoch summary\n",
    "\n",
    "    # Save model checkpoint after each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': u_net_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_epoch_loss,\n",
    "    }, f'unet_model_epoch_{epoch+1}.pth')  # Save model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference process in DDPM involves reversing the diffusion process to generate new samples. The steps are as follows:\n",
    "\n",
    "1. Sample from the prior: Start by sampling $x_T \\sim \\mathcal{N}(0, I)$, which is the prior distribution.\n",
    "\n",
    "2. Reverse the diffusion process: Sequentially get $x_{t-1}$ from xt for $t = T, T-1, \\ldots, 1$ using\n",
    "   \n",
    "    $$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon(x_t, t)) + \\sigma_t z$$\n",
    "\n",
    "    where $z \\sim \\mathcal{N}(0, I)$ and $\\sigma_t^2 = \\beta_t = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}(1-\\alpha_t)$\n",
    "\n",
    "3. Obtain the final sample: The final sample x0 is obtained after completing the reverse process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_beta_schedule(num_timesteps):\n",
    "    \"\"\"\n",
    "    Get the beta schedule for the diffusion process\n",
    "\n",
    "    Args:\n",
    "        num_timesteps (int): Number of timesteps in diffusion process\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Beta schedule tensor\n",
    "    \"\"\"\n",
    "    # Create linear schedule from BETA_START to BETA_END\n",
    "    return torch.linspace(BETA_START, BETA_END, num_timesteps).to('cuda')  # Return beta schedule on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sample(model, num_timesteps, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate a new latent sample optimized for VQVAE latent space\n",
    "\n",
    "    Args:\n",
    "        model: Trained UNet model for noise prediction\n",
    "        num_timesteps: Number of diffusion steps\n",
    "        device: Computing device\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Generated latent sample\n",
    "    \"\"\"\n",
    "    # Initialize with smaller noise scale for better VQVAE compatibility\n",
    "    x_t = torch.randn(1, 128, 8, 8).to(device) * 0.25  # Reduced initial noise scale\n",
    "\n",
    "    # Get diffusion parameters\n",
    "    betas = get_beta_schedule(num_timesteps)  # Get noise schedule\n",
    "    alphas = 1 - betas  # Calculate alphas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)  # Calculate cumulative product\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for t in reversed(range(num_timesteps)):  # Reverse diffusion process\n",
    "            t_tensor = torch.tensor([t], device=device)  # Current timestep\n",
    "            predicted_noise = model(x_t, t_tensor)  # Predict noise\n",
    "\n",
    "            # Get current timestep parameters\n",
    "            alpha_t = alphas[t]  # Current alpha\n",
    "            alpha_t_bar = alphas_cumprod[t]  # Current cumulative alpha\n",
    "            beta_t = betas[t]  # Current beta\n",
    "\n",
    "            # Calculate noise scale with reduced magnitude\n",
    "            sigma_t = torch.sqrt(beta_t) * 0.3  # Reduced noise scale\n",
    "\n",
    "            # Add noise only for non-final steps\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x_t) * sigma_t  # Generate scaled noise\n",
    "            else:\n",
    "                noise = 0  # No noise at final step\n",
    "\n",
    "            # Reverse diffusion step\n",
    "            x_t = (1 / torch.sqrt(alpha_t)) * (\n",
    "                x_t - ((1 - alpha_t) / torch.sqrt(1 - alpha_t_bar)) * predicted_noise\n",
    "            ) + noise\n",
    "\n",
    "            # Clamp values to prevent extremes\n",
    "            x_t = torch.clamp(x_t, -2, 2)  # Keep values in reasonable range\n",
    "\n",
    "    return x_t  # Return generated sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the trained model and generate a single sample\n",
    "model_path = r\"D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\unet_model_epoch_50.pth\"  # Path to saved model checkpoint\n",
    "loaded_unet_model = UNet().to('cuda')  # Initialize model and move to GPU\n",
    "\n",
    "# Load the checkpoint with weights_only=True for security\n",
    "checkpoint = torch.load(model_path, weights_only=True)  # Load checkpoint safely\n",
    "\n",
    "# Handle both direct state dict and nested checkpoint formats\n",
    "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:  # Check if nested format\n",
    "    loaded_unet_model.load_state_dict(checkpoint['model_state_dict'])  # Load nested state dict\n",
    "else:\n",
    "    loaded_unet_model.load_state_dict(checkpoint)  # Load direct state dict\n",
    "\n",
    "# Set model to evaluation mode\n",
    "loaded_unet_model.eval()  # Important for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a single sample\n",
    "num_timesteps = 1000  # Number of timesteps for diffusion\n",
    "\n",
    "# Load the saved VQVAE model\n",
    "vqvae_model = VQVAE(embedding_dim=embedding_dim, num_embeddings=num_embeddings).to('cuda')  # Initialize model\n",
    "vqvae_checkpoint = torch.load('vqvae_model_epoch_30.pth', weights_only=True)  # Load checkpoint\n",
    "vqvae_model.load_state_dict(vqvae_checkpoint)  # Load state dictionary\n",
    "vqvae_model.eval()  # Set to evaluation mode\n",
    "\n",
    "num_samples = 100  # Number of samples to generate # Define number of samples to generate in loop\n",
    "\n",
    "for i in range(num_samples):  # Loop through desired number of samples\n",
    "    generated_sample = generate_sample(loaded_unet_model, num_timesteps, device='cuda')  # Generate one sample\n",
    "\n",
    "    # Print sample dimensions\n",
    "    print(f\"Generated sample {i+1} dimensions: {generated_sample.shape}\")  # Display tensor dimensions for current sample\n",
    "\n",
    "    # Move generated sample to same device as VQVAE\n",
    "    generated_sample = generated_sample.to('cuda')  # Ensure sample is on GPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass through VQ layer first to quantize the latents\n",
    "        _, quantized, _, _ = vqvae_model.vq_layer(generated_sample)  # Quantize the generated latents using VQVAE quantizer\n",
    "\n",
    "        # Then decode the quantized representation\n",
    "        decoded_image = vqvae_model.decoder(quantized)  # Decode the quantized latents into an image\n",
    "\n",
    "    # Decode the generated latent sample\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        decoded_image = vqvae_model.decoder(generated_sample)  # Pass through VQVAE decoder\n",
    "\n",
    "    # Save the generated image with unique name\n",
    "    save_path = f\"generated_diffusion_sample_{i+1}.png\"  # Define save path with sample number\n",
    "    torchvision.utils.save_image(decoded_image[0], save_path)  # Save image to file\n",
    "    print(f\"Image {i+1} saved to {save_path}\")  # Confirm save location for current sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Resize images to Inception v3 input size\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats ie we will also preprocess the data in same way as the original imagenet dataset that was used to train Inception v3\n",
    "])  # Define preprocessing steps for Inception v3 input\n",
    "\n",
    "def prepare_inception_input(images):\n",
    "    return preprocess(images)  # Apply preprocessing to the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3  # Import inception_v3 model from torchvision\n",
    "\n",
    "def load_inception_model():\n",
    "    \"\"\"\n",
    "    Loads and prepares a pre-trained Inception v3 model for feature extraction\n",
    "\n",
    "    Returns:\n",
    "        model: Modified Inception v3 model with final classification layer removed\n",
    "    \"\"\"\n",
    "    model = inception_v3(pretrained=True, transform_input=False)  # Load pre-trained Inception v3 model without input transformation\n",
    "    model.fc = torch.nn.Identity()  # Removes the last fully connected layer\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model.to(device)  # Move the model to the same device as our GAN\n",
    "\n",
    "inception_model = load_inception_model()  # Load and prepare the modified Inception v3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import glob  # Import glob module for file path operations\n",
    "\n",
    "def extract_inception_features(image_paths):\n",
    "    \"\"\"\n",
    "    Extracts features from images using Inception v3 model\n",
    "\n",
    "    Args:\n",
    "        image_paths: List of paths to image files\n",
    "\n",
    "    Returns:\n",
    "        features: Tensor of extracted features from all images\n",
    "    \"\"\"\n",
    "    all_features = []  # Initialize list to store features from all images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to tensor\n",
    "        transforms.Resize((299, 299)),  # Resize to inception input size\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "    ])  # Define transformations for input images\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for img_path in image_paths:  # Process each image\n",
    "            img = Image.open(img_path).convert('RGB')  # Load and convert image to RGB\n",
    "            img_tensor = transform(img).unsqueeze(0).to(device)  # Transform and add batch dimension\n",
    "            features = inception_model(img_tensor)  # Extract features using inception model\n",
    "            all_features.append(features)  # Store features for current image\n",
    "\n",
    "    return torch.cat(all_features, dim=0)  # Concatenate all features into single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get list of generated image paths\n",
    "generated_image_paths = glob.glob(r\"D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\generated_samples\\*.png\")  # Get paths of all PNG files in directory\n",
    "\n",
    "# Extract features from generated images\n",
    "generated_features = extract_inception_features(generated_image_paths)  # Extract inception features from generated images\n",
    "print(f\"Extracted features from {len(generated_image_paths)} generated images\")  # Print number of processed images\n",
    "print(f\"Feature shape: {generated_features.shape}\")  # Print shape of extracted features\n",
    "\n",
    "# Calculate statistics of generated features\n",
    "generated_mean = torch.mean(generated_features, dim=0)  # Calculate mean across all samples for each feature dimension\n",
    "generated_cov = torch.cov(generated_features.T)  # Calculate covariance matrix of features across samples\n",
    "\n",
    "print(f\"Generated features mean shape: {generated_mean.shape}\")  # Print shape of mean vector\n",
    "print(f\"Generated features covariance shape: {generated_cov.shape}\")  # Print shape of covariance matrix\n",
    "\n",
    "# Save statistics for later comparison\n",
    "torch.save(generated_mean, 'generated_mean.pt')  # Save mean vector to disk for future use\n",
    "torch.save(generated_cov, 'generated_cov.pt')  # Save covariance matrix to disk for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get list of real image paths and verify they exist\n",
    "real_image_paths = glob.glob(r\"D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\Butterfly dataset\\*.jpg\")[:100]  # Get paths of 100 JPG files from real dataset\n",
    "if len(real_image_paths) == 0:  # Check if any image paths were found\n",
    "    raise ValueError(\"No image files found in the specified directory\")  # Raise error if no images found\n",
    "\n",
    "# Extract features from real images with error handling\n",
    "try:\n",
    "    real_features = extract_inception_features(real_image_paths)  # Extract inception features from real images\n",
    "    print(f\"Extracted features from {len(real_image_paths)} real images\")  # Print number of processed images\n",
    "    print(f\"Feature shape: {real_features.shape}\")  # Print shape of extracted features\n",
    "\n",
    "    # Calculate statistics of real features\n",
    "    real_mean = torch.mean(real_features, dim=0)  # Calculate mean across all samples for each feature dimension\n",
    "    real_cov = torch.cov(real_features.T)  # Calculate covariance matrix of features across samples\n",
    "\n",
    "    print(f\"Real features mean shape: {real_mean.shape}\")  # Print shape of mean vector\n",
    "    print(f\"Real features covariance shape: {real_cov.shape}\")  # Print shape of covariance matrix\n",
    "\n",
    "    # Save statistics for later comparison\n",
    "    torch.save(real_mean, 'real_mean.pt')  # Save mean vector to disk for future use\n",
    "    torch.save(real_cov, 'real_cov.pt')  # Save covariance matrix to disk for future use\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error processing images: {str(e)}\")  # Print error message if feature extraction fails\n",
    "    print(\"Please verify that all images are valid and accessible\")  # Provide troubleshooting hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_frechet_inception_distance(real_mean, real_cov, generated_mean, generated_cov):\n",
    "    \"\"\"\n",
    "    Calculate the Fréchet Inception Distance (FID) between real and generated image features.\n",
    "\n",
    "    Args:\n",
    "    real_mean (torch.Tensor): Mean of real image features.\n",
    "    real_cov (torch.Tensor): Covariance matrix of real image features.\n",
    "    generated_mean (torch.Tensor): Mean of generated image features.\n",
    "    generated_cov (torch.Tensor): Covariance matrix of generated image features.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated FID score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy for scipy operations\n",
    "    real_mean_np = real_mean.cpu().numpy()  # Convert real mean to numpy array\n",
    "    real_cov_np = real_cov.cpu().numpy()  # Convert real covariance to numpy array\n",
    "    generated_mean_np = generated_mean.cpu().numpy()  # Convert generated mean to numpy array\n",
    "    generated_cov_np = generated_cov.cpu().numpy()  # Convert generated covariance to numpy array\n",
    "\n",
    "    # Calculate squared L2 norm between means\n",
    "    mean_diff = np.sum((real_mean_np - generated_mean_np) ** 2)  # Compute squared difference between means\n",
    "\n",
    "    # Calculate sqrt of product of covariances\n",
    "    covmean = scipy.linalg.sqrtm(real_cov_np.dot(generated_cov_np))  # Compute matrix square root\n",
    "\n",
    "    # Check and correct imaginary parts if necessary\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real  # Take only the real part if result is complex\n",
    "\n",
    "    # Calculate trace term\n",
    "    trace_term = np.trace(real_cov_np + generated_cov_np - 2 * covmean)  # Compute trace of the difference\n",
    "\n",
    "    # Compute FID\n",
    "    fid = mean_diff + trace_term  # Sum up mean difference and trace term\n",
    "\n",
    "    return fid  # Return FID as a Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# Move tensors to CPU and convert to numpy arrays\n",
    "generated_mean_cpu = generated_mean.cpu()  # Move generated mean tensor to CPU\n",
    "generated_cov_cpu = generated_cov.cpu()  # Move generated covariance tensor to CPU\n",
    "real_mean_cpu = real_mean.cpu()  # Move real mean tensor to CPU\n",
    "real_cov_cpu = real_cov.cpu()  # Move real covariance tensor to CPU\n",
    "\n",
    "# Calculate FID score using CPU tensors\n",
    "fid_score = calculate_frechet_inception_distance(real_mean_cpu, real_cov_cpu, generated_mean_cpu, generated_cov_cpu)  # Calculate FID using CPU tensors\n",
    "print(f\"Fréchet Inception Distance: {fid_score:.4f}\")  # Print calculated FID score with 4 decimal places"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
